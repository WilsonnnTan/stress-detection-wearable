{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ace18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, UpSampling1D, MaxPooling1D, AveragePooling1D, ReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREFIX = \"../../dataset/processed/WESAD/\"\n",
    "C_PATH = os.path.join(PATH_PREFIX, \"merged_data\", \"merged_chest_fltr.pkl\")\n",
    "W1_PATH = os.path.join(PATH_PREFIX, \"merged_data\", \"subj_merged_bvp_w.pkl\")\n",
    "W2_PATH = os.path.join(PATH_PREFIX, \"merged_data\", \"subj_merged_eda_temp_w.pkl\")\n",
    "feat_sf700 = ['ecg', 'emg', 'eda', 'temp', 'resp']\n",
    "feat_sf64 = ['bvp']\n",
    "feat_sf4 = ['w_eda', 'w_temp']\n",
    "sf_chest = 700 #sampling frequency for measurements collected from chest device\n",
    "sf_BVP = 64\n",
    "sf_EDA = 4\n",
    "sf_TEMP = 4\n",
    "\n",
    "window = 0.25 # sampling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b50978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.df_c = pd.read_pickle(C_PATH)\n",
    "        self.df_w1 = pd.read_pickle(W1_PATH)\n",
    "        self.df_w2 = pd.read_pickle(W2_PATH)\n",
    "        self.df_w1 = self.df_w1[self.df_w1[\"label\"].isin([1,2,3])]\n",
    "        self.df_w2 = self.df_w2[self.df_w2[\"label\"].isin([1,2,3])]\n",
    "        \n",
    "        self.batch_size = int(sf_chest*window) \n",
    "        self.batch_size_bvp =  int(sf_BVP*window) \n",
    "        self.batch_size_eda =  int(sf_EDA*window) \n",
    "        self.batch_size_temp =  int(sf_TEMP*window) \n",
    "\n",
    "        self.ids = self.df_c[\"sid\"].unique().astype(int)\n",
    "        self.K = len(self.df_c[\"label\"].unique())\n",
    "        \n",
    "    def one_hot_enc(self, r, k):\n",
    "        new_r = np.zeros((r.shape[0],k))\n",
    "        for i, val in enumerate(r):\n",
    "            new_r[i, val-1] = 1\n",
    "\n",
    "        return new_r\n",
    "    \n",
    "    def get_data(self, test_id, v_batch_size, v_feat_list, df):\n",
    "        \n",
    "        cnt=0\n",
    "        \n",
    "        for j in self.ids:\n",
    "            df_s = df[df[\"sid\"] == j]\n",
    "\n",
    "            n = (len(df_s)//v_batch_size)*v_batch_size\n",
    "            df_s = df_s[:n]\n",
    "            s = StandardScaler().fit_transform(df_s[v_feat_list])\n",
    "            s = s.reshape(int(s.shape[0]/v_batch_size), s.shape[1],  v_batch_size)\n",
    "\n",
    "            lbl_m = np.zeros((s.shape[0],1))\n",
    "            lbl = df_s[\"label\"].values.astype(int)\n",
    "            for i in range(s.shape[0]):\n",
    "                lbl_m[i] = int((stats.mode(lbl[i * v_batch_size : (i + 1) * v_batch_size - 1]))[0].squeeze())\n",
    "            y_k = lbl_m.astype(int)\n",
    "            s_y = self.one_hot_enc(lbl_m.astype(int), self.K).astype(int)\n",
    "            #print(\"subject \", j)\n",
    "            #print(s.shape, s_y.shape)\n",
    "            if j==test_id:\n",
    "                x_test = s\n",
    "                y_test = s_y\n",
    "                yk_test = y_k\n",
    "            else:\n",
    "                if cnt:\n",
    "                    merged = np.concatenate((merged, s), axis=0)\n",
    "                    merged_y = np.concatenate((merged_y, s_y), axis=0)\n",
    "                    merged_yk = np.concatenate((merged_yk, y_k), axis=0)\n",
    "                else:\n",
    "                    merged = s\n",
    "                    merged_y = s_y\n",
    "                    merged_yk = y_k\n",
    "                cnt +=1\n",
    "\n",
    "\n",
    "        print (\"merged train:\", merged.shape, merged_y.shape)\n",
    "        print (\"merged test :\", x_test.shape, y_test.shape)\n",
    "        return merged, merged_y, x_test, y_test, merged_yk, yk_test\n",
    "\n",
    "    # train and store autoencoder model for chest modalities\n",
    "    def train_model_c(self):   \n",
    "        # leave one out method\n",
    "        scores = []\n",
    "\n",
    "        for sid in self.ids:\n",
    "            x_train, y_train, x_test, y_test, yk, yk_test = self.get_data (test_id =sid, \n",
    "                                                                       v_batch_size=sf_chest, \n",
    "                                                                       v_feat_list=feat_sf700, \n",
    "                                                                       df=self.df_c)\n",
    "\n",
    "            encoder, model = self.autoenc_model_chest(v_batch_size=sf_chest, n_feat=len(feat_sf700))\n",
    "            model.compile(optimizer=RMSprop(learning_rate=0.00025), loss=\"mse\")\n",
    "            history = model.fit(x_train, x_train, epochs=10)\n",
    "            m_name = \"../../trained_models/WESAD/chest_encoder_model/encoder_loso\"+str(sid)+\".h5\"\n",
    "\n",
    "            encoder.save(m_name)\n",
    "            print(\"saved \", m_name)\n",
    "            \n",
    "    def autoenc_model_w1(self, v_batch_size, n_feat):\n",
    "    \n",
    "        input_sig = Input(shape=(n_feat, v_batch_size))\n",
    "        x = Conv1D(v_batch_size,6, activation='relu', padding='same')(input_sig)\n",
    "        x1 = BatchNormalization()(x)\n",
    "        x2 = Conv1D(v_batch_size,3, activation='relu', padding='same')(x1)\n",
    "        flat = Flatten()(x2)\n",
    "\n",
    "        encoded = Dense(40, activation='relu')(flat)\n",
    "\n",
    "        encoder = Model(input_sig, encoded)\n",
    "\n",
    "        d1 = Dense(v_batch_size*n_feat)(encoded)\n",
    "        d2 = Reshape((n_feat,v_batch_size))(d1)\n",
    "        d3 = Conv1D(v_batch_size,3, activation='relu', padding='same')(d2)\n",
    "        d4 = BatchNormalization()(d3)\n",
    "        d5 = Conv1D(v_batch_size,6, activation='sigmoid', padding='same',  name='reconst_output')(d4)\n",
    "\n",
    "        model= Model(input_sig, d5)\n",
    "\n",
    "        return encoder, model\n",
    "    \n",
    "    def autoenc_model_w2(self, v_batch_size, n_feat):\n",
    "        \n",
    "        input_sig = Input(shape=(n_feat, v_batch_size))\n",
    "        x = Conv1D(v_batch_size,4, activation='relu', padding='same')(input_sig)\n",
    "\n",
    "        x1 = BatchNormalization()(x)\n",
    "        flat = Flatten()(x1)\n",
    "        encoded = Dense(4, activation='relu')(flat)\n",
    "\n",
    "        encoder = Model(input_sig, encoded)\n",
    "\n",
    "        d1 = Dense(v_batch_size*n_feat)(encoded)\n",
    "        d2 = Reshape((n_feat,v_batch_size))(d1)\n",
    "        d5 = Conv1D(v_batch_size,4, activation='sigmoid', padding='same',  name='reconst_output')(d2)\n",
    "\n",
    "        model= Model(input_sig, d5)\n",
    "\n",
    "        return encoder, model   \n",
    "    \n",
    "    def autoenc_model_chest(self, v_batch_size, n_feat):\n",
    "    \n",
    "        input_sig = Input(shape=(n_feat, v_batch_size))\n",
    "        x = Conv1D(v_batch_size,6, activation='relu', padding='same')(input_sig)\n",
    "\n",
    "        x1 = BatchNormalization()(x)\n",
    "        x2 = Conv1D(v_batch_size,3, activation='relu', padding='same')(x1)\n",
    "        flat = Flatten()(x2)\n",
    "        \n",
    "        encoded = Dense(80, activation='relu')(flat)\n",
    "\n",
    "        encoder = Model(input_sig, encoded)\n",
    "\n",
    "        d1 = Dense(v_batch_size*n_feat)(encoded)\n",
    "        d2 = Reshape((n_feat,v_batch_size))(d1)\n",
    "        d3 = Conv1D(v_batch_size,3, activation='relu', padding='same')(d2)\n",
    "        d4 = BatchNormalization()(d3)\n",
    "        d5 = Conv1D(v_batch_size,6, activation='sigmoid', padding='same')(d4)\n",
    "\n",
    "        model= Model(input_sig, d5)\n",
    "\n",
    "        return encoder, model\n",
    "    \n",
    "    def extract_features(self):\n",
    "        for sid in self.ids:\n",
    "            print(\"============= test subject \" +str(sid)+ \" ==================\")\n",
    "            x_train, y_train, x_test, y_test, yk, yk_test = self.get_data(test_id=sid,\n",
    "                                                                        v_batch_size=sf_chest,\n",
    "                                                                        v_feat_list=feat_sf700,\n",
    "                                                                        df=self.df_c)\n",
    "            x_trainw1, y_trainw1, x_testw1, y_testw1, yk1w1, yk_test1w1 = self.get_data(test_id=sid,\n",
    "                                                                                    v_batch_size=sf_BVP,\n",
    "                                                                                    v_feat_list=feat_sf64,\n",
    "                                                                                    df=self.df_w1)\n",
    "            x_trainw2, y_trainw2, x_testw2, y_testw2, yk2w2, yk_test2w2 = self.get_data(test_id=sid,\n",
    "                                                                                    v_batch_size=sf_EDA,\n",
    "                                                                                    v_feat_list=feat_sf4,\n",
    "                                                                                    df=self.df_w2)\n",
    "\n",
    "            # Train w1 autoencoder\n",
    "            encoderw1, modelw1 = self.autoenc_model_w1(v_batch_size=sf_BVP, n_feat=len(feat_sf64))\n",
    "            modelw1.compile(optimizer=RMSprop(learning_rate=0.00025), loss=\"mse\")\n",
    "            modelw1.fit(x_trainw1, x_trainw1, epochs=4, verbose=0)\n",
    "\n",
    "            emb_trainw1 = encoderw1.predict(x_trainw1)\n",
    "            emb_testw1 = encoderw1.predict(x_testw1)\n",
    "\n",
    "            # Train w2 autoencoder\n",
    "            encoderw2, modelw2 = self.autoenc_model_w2(v_batch_size=sf_EDA, n_feat=len(feat_sf4))\n",
    "            modelw2.compile(optimizer=RMSprop(learning_rate=0.00025), loss=\"mse\")\n",
    "            modelw2.fit(x_trainw2, x_trainw2, epochs=4, verbose=0)\n",
    "\n",
    "            emb_trainw2 = encoderw2.predict(x_trainw2)\n",
    "            emb_testw2 = encoderw2.predict(x_testw2)\n",
    "\n",
    "            # Load pre-trained chest encoder\n",
    "            m_name = \"../../trained_models/WESAD/chest_encoder/encoder_loso\"+str(sid)+\".h5\"\n",
    "            encoder = tf.keras.models.load_model(m_name)\n",
    "            print(\"loaded: \", m_name)\n",
    "\n",
    "            emb_train = encoder.predict(x_train)\n",
    "            emb_test = encoder.predict(x_test)\n",
    "\n",
    "            # Align lengths\n",
    "            last_inx_train = min(emb_trainw1.shape[0], emb_trainw2.shape[0], emb_train.shape[0])\n",
    "            last_inx_test = min(emb_testw1.shape[0], emb_testw2.shape[0], emb_test.shape[0])\n",
    "\n",
    "            # Concatenate embeddings (without label)\n",
    "            emb_train_all = np.concatenate((emb_train[:last_inx_train],\n",
    "                                            emb_trainw1[:last_inx_train],\n",
    "                                            emb_trainw2[:last_inx_train]), axis=1)\n",
    "            emb_test_all = np.concatenate((emb_test[:last_inx_test],\n",
    "                                        emb_testw1[:last_inx_test],\n",
    "                                        emb_testw2[:last_inx_test]), axis=1)\n",
    "\n",
    "            # Labels for stacking\n",
    "            y_train_final = yk[:last_inx_train].squeeze()\n",
    "            y_test_final = yk_test[:last_inx_test].squeeze()\n",
    "\n",
    "            # Save features and labels separately for stacking\n",
    "            train_feat_file = f\"{PATH_PREFIX}features/train/feat_loso{sid}_X.pkl\"\n",
    "            train_label_file = f\"{PATH_PREFIX}features/train/feat_loso{sid}_y.pkl\"\n",
    "            test_feat_file = f\"{PATH_PREFIX}features/test/feat_loso{sid}_X.pkl\"\n",
    "            test_label_file = f\"{PATH_PREFIX}features/test/feat_loso{sid}_y.pkl\"\n",
    "\n",
    "            pd.DataFrame(emb_train_all).to_pickle(train_feat_file)\n",
    "            pd.DataFrame(y_train_final).to_pickle(train_label_file)\n",
    "            pd.DataFrame(emb_test_all).to_pickle(test_feat_file)\n",
    "            pd.DataFrame(y_test_final).to_pickle(test_label_file)\n",
    "\n",
    "            print(f\"Saved features and labels for LOSO subject {sid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20061f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = autoencoder()\n",
    "ae.train_model_c()\n",
    "ae.extract_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
